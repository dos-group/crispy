{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea66905d",
   "metadata": {},
   "source": [
    "# Evaluation of Crispy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db7a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e55baab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len(jobs) = 16'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cluster = pd.read_csv('arrow_cluster_jobs.csv')\n",
    "df_local = pd.read_csv('crispy_local_jobs.csv')\n",
    "\n",
    "jobs = list(sorted(set(df_cluster['job'])))\n",
    "\n",
    "f\"{len(jobs) = }\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6eab976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>input_size</th>\n",
       "      <th>cost</th>\n",
       "      <th>cost_norm</th>\n",
       "      <th>scaleout</th>\n",
       "      <th>mtype</th>\n",
       "      <th>cores</th>\n",
       "      <th>total_cores</th>\n",
       "      <th>memory</th>\n",
       "      <th>total_memory</th>\n",
       "      <th>...</th>\n",
       "      <th>agg_disk.%util</th>\n",
       "      <th>agg_network.rxpck/s</th>\n",
       "      <th>agg_network.txpck/s</th>\n",
       "      <th>agg_network.rxkB/s</th>\n",
       "      <th>agg_network.txkB/s</th>\n",
       "      <th>agg_network.rxcmp/s</th>\n",
       "      <th>agg_network.txcmp/s</th>\n",
       "      <th>agg_network.rxmcst/s</th>\n",
       "      <th>agg_network.%ifutil</th>\n",
       "      <th>bread</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1722.228000</td>\n",
       "      <td>100000000000</td>\n",
       "      <td>0.746299</td>\n",
       "      <td>1.878088</td>\n",
       "      <td>12</td>\n",
       "      <td>r4.large</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>15.25</td>\n",
       "      <td>183.0</td>\n",
       "      <td>...</td>\n",
       "      <td>265556.91</td>\n",
       "      <td>5948741.33</td>\n",
       "      <td>3588376.43</td>\n",
       "      <td>35888301.26</td>\n",
       "      <td>35940356.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.144086e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7040.141403</td>\n",
       "      <td>401636367057</td>\n",
       "      <td>1.955595</td>\n",
       "      <td>5.194055</td>\n",
       "      <td>10</td>\n",
       "      <td>m4.large</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>8.00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1396.29</td>\n",
       "      <td>158224.97</td>\n",
       "      <td>161207.31</td>\n",
       "      <td>845914.27</td>\n",
       "      <td>576545.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.24</td>\n",
       "      <td>2.143243e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>367.160000</td>\n",
       "      <td>93803644984</td>\n",
       "      <td>0.244773</td>\n",
       "      <td>2.146511</td>\n",
       "      <td>24</td>\n",
       "      <td>m4.large</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>8.00</td>\n",
       "      <td>192.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7563.72</td>\n",
       "      <td>465252.08</td>\n",
       "      <td>430435.82</td>\n",
       "      <td>2296895.30</td>\n",
       "      <td>1447771.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>259.95</td>\n",
       "      <td>1.504549e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>560.571000</td>\n",
       "      <td>2993586581</td>\n",
       "      <td>0.498285</td>\n",
       "      <td>1.733598</td>\n",
       "      <td>16</td>\n",
       "      <td>c4.xlarge</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>7.50</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5021.82</td>\n",
       "      <td>944174.28</td>\n",
       "      <td>948309.69</td>\n",
       "      <td>4887539.75</td>\n",
       "      <td>4872704.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>539.47</td>\n",
       "      <td>7.058923e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1624.248000</td>\n",
       "      <td>240005901008</td>\n",
       "      <td>1.443776</td>\n",
       "      <td>5.501142</td>\n",
       "      <td>8</td>\n",
       "      <td>m4.2xlarge</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>32.00</td>\n",
       "      <td>256.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6134.12</td>\n",
       "      <td>413545.07</td>\n",
       "      <td>377574.86</td>\n",
       "      <td>2542939.24</td>\n",
       "      <td>1373500.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275.63</td>\n",
       "      <td>2.532499e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      duration    input_size      cost  cost_norm  scaleout       mtype  \\\n",
       "0  1722.228000  100000000000  0.746299   1.878088        12    r4.large   \n",
       "1  7040.141403  401636367057  1.955595   5.194055        10    m4.large   \n",
       "2   367.160000   93803644984  0.244773   2.146511        24    m4.large   \n",
       "3   560.571000    2993586581  0.498285   1.733598        16   c4.xlarge   \n",
       "4  1624.248000  240005901008  1.443776   5.501142         8  m4.2xlarge   \n",
       "\n",
       "   cores  total_cores  memory  total_memory  ... agg_disk.%util  \\\n",
       "0      2           24   15.25         183.0  ...      265556.91   \n",
       "1      2           20    8.00          80.0  ...        1396.29   \n",
       "2      2           48    8.00         192.0  ...        7563.72   \n",
       "3      4           64    7.50         120.0  ...        5021.82   \n",
       "4      8           64   32.00         256.0  ...        6134.12   \n",
       "\n",
       "  agg_network.rxpck/s agg_network.txpck/s agg_network.rxkB/s  \\\n",
       "0          5948741.33          3588376.43        35888301.26   \n",
       "1           158224.97           161207.31          845914.27   \n",
       "2           465252.08           430435.82         2296895.30   \n",
       "3           944174.28           948309.69         4887539.75   \n",
       "4           413545.07           377574.86         2542939.24   \n",
       "\n",
       "   agg_network.txkB/s  agg_network.rxcmp/s  agg_network.txcmp/s  \\\n",
       "0         35940356.62                  0.0                  0.0   \n",
       "1           576545.70                  0.0                  0.0   \n",
       "2          1447771.15                  0.0                  0.0   \n",
       "3          4872704.93                  0.0                  0.0   \n",
       "4          1373500.34                  0.0                  0.0   \n",
       "\n",
       "   agg_network.rxmcst/s  agg_network.%ifutil         bread  \n",
       "0                   0.0                 0.00  3.144086e+07  \n",
       "1                   0.0                93.24  2.143243e+05  \n",
       "2                   0.0               259.95  1.504549e+07  \n",
       "3                   0.0               539.47  7.058923e+04  \n",
       "4                   0.0               275.63  2.532499e+07  \n",
       "\n",
       "[5 rows x 160 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cluster.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cd23e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(job):\n",
    "    df_train = df_local[df_local['job'] == job]\n",
    "    X_train = df_train['dataset_size'].to_numpy().reshape(-1,1)\n",
    "    y_train = df_train['max_memory_used'].to_numpy()\n",
    "    \n",
    "    df_test = df_cluster[df_cluster['job'] == job]\n",
    "    X_test = [[df_test.iloc[0]['input_size']]]\n",
    "    \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ce3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_overhead = {'hadoop': 2e9, 'spark': 2.5e9}  # Ubuntu + framework\n",
    "\n",
    "def possible_configs(job, mem_req=0):\n",
    "    filter1 = df_cluster['job'] == job \n",
    "    total_mem_req =   mem_req + df_cluster['scaleout'] \\\n",
    "                    * df_cluster['framework'].map(lambda x: mem_overhead[x])\n",
    "    filter2 = df_cluster['total_memory']*1e9 > total_mem_req\n",
    "    df_configs = df_cluster[filter1 & filter2][['mtype', 'scaleout']]\n",
    "    return set(df_configs.itertuples(index=False, name=None))\n",
    "    \n",
    "\n",
    "def best_config_for_all_other_jobs(job, mem_req=0):\n",
    "    algorithm, framework, dataset = job.split('_')\n",
    "    config_candidates = possible_configs(job, mem_req)\n",
    "    if not config_candidates: return None\n",
    "    \n",
    "    same_framework      = df_cluster['framework'] == framework\n",
    "    different_algorithm = df_cluster['algorithm'] != algorithm\n",
    "    all_other_jobs = df_cluster[ same_framework & different_algorithm ]\n",
    "    \n",
    "    configs = collections.defaultdict(list)\n",
    "    for i, row in all_other_jobs.iterrows():\n",
    "        config = (row['mtype'], row['scaleout'])\n",
    "        if config in config_candidates: \n",
    "            configs[config] += [row['cost_norm']]\n",
    "    \n",
    "    configs = [(sum(v),)+k for k, v in configs.items()]  # (cum_cost, mtype, scaleout)\n",
    "    return sorted(configs)[0][1:]\n",
    "\n",
    "bfa = best_config_for_all_other_jobs\n",
    "\n",
    "\n",
    "def crispy(job):\n",
    "    X_train, y_train, X_test = get_train_data(job)\n",
    "    model = LR()\n",
    "    model.fit(X_train,y_train)\n",
    "    mem_req = model.predict(X_test)[0] if model.score(X_train, y_train) > .99 else 0\n",
    "    return bfa(job, mem_req) or bfa(job, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1dd6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job                       Crispy Selected Configuration\n",
      "-------------------------------------------------------\n",
      "bayes_spark_bigdata          ('c4.large', 4)\n",
      "bayes_spark_huge             ('r4.2xlarge', 8)\n",
      "join_spark_bigdata           ('c4.large', 4)\n",
      "join_spark_huge              ('c4.large', 4)\n",
      "kmeans_spark_bigdata         ('r4.2xlarge', 10)\n",
      "kmeans_spark_huge            ('r4.xlarge', 10)\n",
      "linear_spark_bigdata         ('m4.xlarge', 8)\n",
      "linear_spark_huge            ('c4.large', 4)\n",
      "lr_spark_bigdata             ('c4.large', 6)\n",
      "lr_spark_huge                ('c4.large', 4)\n",
      "pagerank_hadoop_bigdata      ('c4.large', 4)\n",
      "pagerank_hadoop_huge         ('c4.large', 4)\n",
      "pagerank_spark_bigdata       ('r4.xlarge', 8)\n",
      "pagerank_spark_huge          ('r4.large', 4)\n",
      "terasort_hadoop_bigdata      ('c4.large', 6)\n",
      "terasort_hadoop_huge         ('c4.large', 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Job':25s} Crispy Selected Configuration\\n{'-'*55}\")\n",
    "for job in jobs:\n",
    "    print(f\"{job:28s} {crispy(job)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c23adb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_eval(job, predictors):\n",
    "    \n",
    "    job_df =  df_cluster[df_cluster['job'] == job]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results['Random'] = float(job_df['cost_norm'].mean())\n",
    "    results['Medium'] = float(job_df[  (job_df['mtype']=='m4.xlarge') \n",
    "                                     & (job_df['scaleout']==12) ]\n",
    "                              ['cost_norm'].mean())\n",
    "    \n",
    "    for predictor_name, predictor in predictors:\n",
    "        mtype, scaleout = predictor(job)\n",
    "        predicted = float(job_df[  (job_df['mtype']==mtype) \n",
    "                                 & (job_df['scaleout'] == scaleout)]['cost_norm'])\n",
    "        results[predictor_name] = predicted\n",
    "        \n",
    "    return results\n",
    "\n",
    "def full_eval():\n",
    "    \n",
    "    predictors = [best_config_for_all_other_jobs, crispy]\n",
    "    predictor_names = ['BFA', 'Crispy']\n",
    "    \n",
    "    df = pd.DataFrame(columns=['job']+predictor_names)\n",
    "    \n",
    "    for job in jobs:\n",
    "        eval_results = single_eval(job, zip(predictor_names, predictors))\n",
    "        df = df.append({**{'job':job}, **eval_results}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9149750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selector | Cost (norm)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random    2.348822\n",
       "Medium    2.009813\n",
       "BFA       1.769217\n",
       "Crispy    1.337519\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = full_eval()[['job', 'Random', 'Medium', 'BFA', 'Crispy']]\n",
    "print(f\"{'Selector':8s} | Cost (norm)\")\n",
    "df_results.median()\n",
    "df_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47037718",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>Random</th>\n",
       "      <th>Medium</th>\n",
       "      <th>BFA</th>\n",
       "      <th>Crispy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bayes_spark_bigdata</td>\n",
       "      <td>1.283399</td>\n",
       "      <td>1.173069</td>\n",
       "      <td>1.095362</td>\n",
       "      <td>1.095362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bayes_spark_huge</td>\n",
       "      <td>1.408278</td>\n",
       "      <td>1.354763</td>\n",
       "      <td>1.203942</td>\n",
       "      <td>1.000503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>join_spark_bigdata</td>\n",
       "      <td>1.848328</td>\n",
       "      <td>1.567345</td>\n",
       "      <td>1.050709</td>\n",
       "      <td>1.050709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>join_spark_huge</td>\n",
       "      <td>2.548079</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kmeans_spark_bigdata</td>\n",
       "      <td>3.476274</td>\n",
       "      <td>2.787279</td>\n",
       "      <td>3.991071</td>\n",
       "      <td>1.156992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kmeans_spark_huge</td>\n",
       "      <td>3.339827</td>\n",
       "      <td>3.152283</td>\n",
       "      <td>4.777844</td>\n",
       "      <td>1.175872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear_spark_bigdata</td>\n",
       "      <td>1.353124</td>\n",
       "      <td>1.210515</td>\n",
       "      <td>1.133372</td>\n",
       "      <td>1.133372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>linear_spark_huge</td>\n",
       "      <td>3.196385</td>\n",
       "      <td>3.718082</td>\n",
       "      <td>3.121199</td>\n",
       "      <td>3.121199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lr_spark_bigdata</td>\n",
       "      <td>3.547521</td>\n",
       "      <td>2.502524</td>\n",
       "      <td>1.731832</td>\n",
       "      <td>1.731832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lr_spark_huge</td>\n",
       "      <td>5.210249</td>\n",
       "      <td>4.104707</td>\n",
       "      <td>2.487412</td>\n",
       "      <td>2.487412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pagerank_hadoop_bigdata</td>\n",
       "      <td>1.664086</td>\n",
       "      <td>1.499470</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pagerank_hadoop_huge</td>\n",
       "      <td>2.041895</td>\n",
       "      <td>1.867101</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pagerank_spark_bigdata</td>\n",
       "      <td>1.429539</td>\n",
       "      <td>1.226108</td>\n",
       "      <td>1.394448</td>\n",
       "      <td>1.246909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pagerank_spark_huge</td>\n",
       "      <td>1.797276</td>\n",
       "      <td>1.351337</td>\n",
       "      <td>1.204038</td>\n",
       "      <td>1.083902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>terasort_hadoop_bigdata</td>\n",
       "      <td>1.646174</td>\n",
       "      <td>1.363079</td>\n",
       "      <td>1.116246</td>\n",
       "      <td>1.116246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>terasort_hadoop_huge</td>\n",
       "      <td>1.790713</td>\n",
       "      <td>1.269539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        job    Random    Medium       BFA    Crispy\n",
       "0       bayes_spark_bigdata  1.283399  1.173069  1.095362  1.095362\n",
       "1          bayes_spark_huge  1.408278  1.354763  1.203942  1.000503\n",
       "2        join_spark_bigdata  1.848328  1.567345  1.050709  1.050709\n",
       "3           join_spark_huge  2.548079       NaN  1.000000  1.000000\n",
       "4      kmeans_spark_bigdata  3.476274  2.787279  3.991071  1.156992\n",
       "5         kmeans_spark_huge  3.339827  3.152283  4.777844  1.175872\n",
       "6      linear_spark_bigdata  1.353124  1.210515  1.133372  1.133372\n",
       "7         linear_spark_huge  3.196385  3.718082  3.121199  3.121199\n",
       "8          lr_spark_bigdata  3.547521  2.502524  1.731832  1.731832\n",
       "9             lr_spark_huge  5.210249  4.104707  2.487412  2.487412\n",
       "10  pagerank_hadoop_bigdata  1.664086  1.499470  1.000000  1.000000\n",
       "11     pagerank_hadoop_huge  2.041895  1.867101  1.000000  1.000000\n",
       "12   pagerank_spark_bigdata  1.429539  1.226108  1.394448  1.246909\n",
       "13      pagerank_spark_huge  1.797276  1.351337  1.204038  1.083902\n",
       "14  terasort_hadoop_bigdata  1.646174  1.363079  1.116246  1.116246\n",
       "15     terasort_hadoop_huge  1.790713  1.269539  1.000000  1.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results  # In detail, for each job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e46d7cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job                     Profiling time\n",
      "--------------------------------------\n",
      "bayes_spark_bigdata        373 seconds\n",
      "bayes_spark_huge           369 seconds\n",
      "join_spark_bigdata         136 seconds\n",
      "join_spark_huge            110 seconds\n",
      "kmeans_spark_bigdata       470 seconds\n",
      "kmeans_spark_huge          470 seconds\n",
      "linear_spark_bigdata       372 seconds\n",
      "linear_spark_huge          198 seconds\n",
      "lr_spark_bigdata           675 seconds\n",
      "lr_spark_huge              562 seconds\n",
      "pagerank_hadoop_bigdata    812 seconds\n",
      "pagerank_hadoop_huge       812 seconds\n",
      "pagerank_spark_bigdata    1292 seconds\n",
      "pagerank_spark_huge       1292 seconds\n",
      "terasort_hadoop_bigdata    547 seconds\n",
      "terasort_hadoop_huge       547 seconds\n"
     ]
    }
   ],
   "source": [
    "profiling_times = []\n",
    "\n",
    "print(f\"{'Job':23s} {'Profiling time'}\\n{'-'*38}\")\n",
    "for job in jobs:\n",
    "    job_df = df_local[df_local['job'] == job]\n",
    "    profiling_time = job_df['runtime'].sum()\n",
    "    print(f\"{job:25s} {profiling_time:4d} seconds\")\n",
    "    profiling_times.append(profiling_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c0565a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profiling_times.min()  / 60  =  1.83 minutes\n",
      "profiling_times.max()  / 60  = 21.53 minutes\n",
      "profiling_times.mean() / 60  =  9.41 minutes\n",
      "np.median(profiling_times) / 60  =  8.47 minutes\n"
     ]
    }
   ],
   "source": [
    "profiling_times = np.array(profiling_times)\n",
    "print(f\"{profiling_times.min()  / 60  =  :.2f} minutes\")\n",
    "print(f\"{profiling_times.max()  / 60  = :.2f} minutes\")\n",
    "print(f\"{profiling_times.mean() / 60  =  :.2f} minutes\")\n",
    "print(f\"{np.median(profiling_times) / 60  =  :.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
